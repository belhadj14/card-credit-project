{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe33b331-2a60-49d4-b31f-d4d3e07a97e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================================================================#\n",
    "# Modeling the data\n",
    "\n",
    "# Main Metric\n",
    "#.         Precision\n",
    "#.         Precision\n",
    "\n",
    "#=================================================================================================#\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder,MinMaxScaler,StandardScaler\n",
    "# from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Set styling for better visualization\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Global configuration\n",
    "DATA_ROOT=Path(\"../data/interim/\").resolve()\n",
    "DATASET_DIR=Path.joinpath(DATA_ROOT,'creditcard.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eaf6370-68cf-430f-af68-0ee717b09b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "def load_dataset(path_dir):\n",
    "    df=pd.read_csv(path_dir,sep=\",\")\n",
    "    return df\n",
    "\n",
    "df=load_dataset(DATASET_DIR)\n",
    "X=df.drop('Class',axis=1)\n",
    "y=df['Class'] # Target\n",
    "# Split data\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "#Scaling data\n",
    "scaler=StandardScaler()\n",
    "X_train_scaled=scaler.fit_transform(X_train)\n",
    "X_test_scaled=scaler.fit(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418f2b7e-b863-41e4-aa88-1c3d086e8f06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e00f694b-94d4-426a-877f-694a16f5410b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    284315\n",
       "1       492\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process the target# \n",
    "y.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27fea470-bd79-4ff1-9606-fda715ddf2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple model\n",
    "def create_simple_model(input_shape):\n",
    "    model=tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Input(shape=(input_shape,1)),\n",
    "            tf.keras.layers.Dense(64,activation=\"relu\"),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(32,activation=\"relu\"),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(16,activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
    "        ]\n",
    "        \n",
    "    )\n",
    "\n",
    "    #compile the model\n",
    "    model.compile(\n",
    "        optimizer= tf.keras.optimizers.Adam(learning_rate=initial_lr),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC()]\n",
    "        \n",
    "    )\n",
    "    return  model\n",
    "# Create a simple model\n",
    "def create_fairly_medium_model(input_shape):\n",
    "    model=tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Input(shape=(input_shape,1)),\n",
    "            tf.keras.layers.Dense(64,activation=\"relu\"),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "            tf.keras.layers.Dense(32,activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(2,activation=\"softmax\")\n",
    "        ]\n",
    "        \n",
    "    )\n",
    "\n",
    "    #compile the model\n",
    "    model.compile(\n",
    "        optimizer= tf.keras.optimizers.Adam(),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=['precision']\n",
    "        \n",
    "    )\n",
    "    return  model\n",
    "\n",
    "    # Create a simple model\n",
    "def create_advanced_model(input_shape):\n",
    "    model=tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Input(shape=(input_shape,1)),\n",
    "            tf.keras.layers.Dense(128,activation=\"relu\"),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "            tf.keras.layers.Dense(64,activation=\"relu\"),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "            tf.keras.layers.Dense(64,activation=\"relu\"),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "            tf.keras.layers.Dense(32,activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(32,activation=\"relu\"),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(2,activation=\"softmax\")\n",
    "        ]\n",
    "        \n",
    "    )\n",
    "\n",
    "    #compile the model\n",
    "    model.compile(\n",
    "        optimizer= tf.keras.optimizers.Adam(),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC()]\n",
    "        \n",
    "    )\n",
    "    return  model\n",
    "\n",
    "# setting callbacks functions\n",
    "def set_callbacks():\n",
    "        checkpoint_dir = 'credit_card_checkpoints'\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "          os.makedirs(checkpoint_dir)\n",
    "            \n",
    "        callbacks=[tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,  # Reduce LR by half when plateauing\n",
    "        patience=10,  # Wait for 10 epochs before reducing LR\n",
    "        min_delta=0.0001,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=25,  # Increased patience to allow LR reduction to take effect\n",
    "        min_delta=0.0001,\n",
    "        mode='min',\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(checkpoint_dir, 'best_model.keras'),\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "            \n",
    "        ]\n",
    "        return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f623b650-4e48-413d-bfae-c911dcee7c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227845"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "414e12e1-ede5-4f41-b9b2-dada2394a3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227845,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed449249-eb11-403c-9cf4-6d2b14809285",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'initial_lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m=\u001b[39m\u001b[43mcreate_simple_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 18\u001b[0m, in \u001b[0;36mcreate_simple_model\u001b[1;34m(input_shape)\u001b[0m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m      4\u001b[0m     [\n\u001b[0;32m      5\u001b[0m         tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(input_shape,\u001b[38;5;241m1\u001b[39m)),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#compile the model\u001b[39;00m\n\u001b[0;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m---> 18\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[43minitial_lr\u001b[49m),\n\u001b[0;32m     19\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     20\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mAUC()]\n\u001b[0;32m     21\u001b[0m     \n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m  model\n",
      "\u001b[1;31mNameError\u001b[0m: name 'initial_lr' is not defined"
     ]
    }
   ],
   "source": [
    "model=create_simple_model(X_train_scaled.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa712428-b3ba-4663-9590-91eaefec8cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 1), output.shape=(None, 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mset_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#     return model, history, X_test, y_test\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\dll\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\.conda\\envs\\dll\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py:701\u001b[0m, in \u001b[0;36mbinary_crossentropy\u001b[1;34m(target, output, from_logits)\u001b[0m\n\u001b[0;32m    699\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n\u001b[1;32m--> 701\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    702\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must have the same shape. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    703\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    704\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    705\u001b[0m         )\n\u001b[0;32m    707\u001b[0m output, from_logits \u001b[38;5;241m=\u001b[39m _get_logits(\n\u001b[0;32m    708\u001b[0m     output, from_logits, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSigmoid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    709\u001b[0m )\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_logits:\n",
      "\u001b[1;31mValueError\u001b[0m: Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 1), output.shape=(None, 30)"
     ]
    }
   ],
   "source": [
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=200,\n",
    "        batch_size=64,\n",
    "        validation_split=0.2,\n",
    "        callbacks=set_callbacks(),\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    \n",
    "#     return model, history, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da46e21-9450-4eb1-8233-33971b8137f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_training_history(history):\n",
    "#     \"\"\"Plot training curves including learning rate changes\"\"\"\n",
    "#     fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 5))\n",
    "    \n",
    "#     # Plot loss\n",
    "#     ax1.plot(history.history['loss'], label='Training Loss')\n",
    "#     ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
    "#     ax1.set_title('Model Loss')\n",
    "#     ax1.set_xlabel('Epoch')\n",
    "#     ax1.set_ylabel('Loss (MSE)')\n",
    "#     ax1.legend()\n",
    "#     ax1.grid(True)\n",
    "    \n",
    "#     # Plot MAE\n",
    "#     ax2.plot(history.history['mae'], label='Training MAE')\n",
    "#     ax2.plot(history.history['val_mae'], label='Validation MAE')\n",
    "#     ax2.set_title('Model Mean Absolute Error')\n",
    "#     ax2.set_xlabel('Epoch')\n",
    "#     ax2.set_ylabel('MAE')\n",
    "#     ax2.legend()\n",
    "#     ax2.grid(True)\n",
    "    \n",
    "#     # Plot Learning Rate\n",
    "#     ax3.plot(history.history['learning_rate'], label='Learning Rate')\n",
    "#     ax3.set_title('Learning Rate Over Time')\n",
    "#     ax3.set_xlabel('Epoch')\n",
    "#     ax3.set_ylabel('Learning Rate')\n",
    "#     ax3.set_yscale('log')  # Use log scale for better visualization\n",
    "#     ax3.legend()\n",
    "#     ax3.grid(True)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1dcfd4-21f0-4d91-814d-173e888ca3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Results Evaluation function\n",
    "# def evaluate_results(model, X_test, y_test, history):\n",
    "#     \"\"\"Evaluate and print model results\"\"\"\n",
    "#     # Evaluate on test set\n",
    "#     test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "#     # Get training statistics\n",
    "#     best_epoch = np.argmin(history.history['val_loss']) + 1\n",
    "#     total_epochs = len(history.history['loss'])\n",
    "#     best_val_loss = min(history.history['val_loss'])\n",
    "#     best_val_mae = min(history.history['val_mae'])\n",
    "    \n",
    "#     # Get learning rate statistics\n",
    "#     initial_lr = history.history['learning_rate'][0]\n",
    "#     final_lr = history.history['learning_rate'][-1]\n",
    "#     num_lr_drops = sum(1 for i in range(1, len(history.history['learning_rate']))\n",
    "#                       if history.history['learning_rate'][i] < history.history['learning_rate'][i-1])\n",
    "    \n",
    "#     print(\"\\nModel Performance Summary:\")\n",
    "#     print(\"-\" * 50)\n",
    "#     print(f\"Best validation loss achieved at epoch: {best_epoch}/{total_epochs}\")\n",
    "#     print(f\"Best validation loss (MSE): {best_val_loss:.4f}\")\n",
    "#     print(f\"Best validation MAE: {best_val_mae:.4f}\")\n",
    "#     print(f\"Test loss (MSE): {test_loss:.4f}\")\n",
    "#     print(f\"Test MAE: {test_mae:.4f}\")\n",
    "#     print(\"\\nLearning Rate Summary:\")\n",
    "#     print(f\"Initial learning rate: {initial_lr:.6f}\")\n",
    "#     print(f\"Final learning rate: {final_lr:.6f}\")\n",
    "#     print(f\"Number of learning rate reductions: {num_lr_drops}\")\n",
    "#     print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0852e6c3-f4d6-4960-adab-3de5cf46b9b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dll",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
